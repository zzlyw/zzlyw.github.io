[{"authors":null,"categories":null,"content":"I am a full-time research scientist at Beijing Institute for General Artificial Intelligence (BIGAI). I received my Ph.D. degree (‘19) from Beijing Institute of Technology advised by Prof. Yongtian Wang, and my research work was mainly about the information fusion of physical and virtual worlds, as well as the application of VR/AR/MR for robot learning. During my doctorate study, I worked as a visiting Ph.D. student in the Center for Vision, Cognition, Learning, and Autonomy (VCLA) at UCLA advised by Prof. Song-Chun Zhu from 2017 to 2019, engaging in research related to autonomous robots. My research interest is focused on cognitive reasoning, multi-agent reinforcement learning, and mixed reality for artificial intelligence.\n","date":1614556800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1614556800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://zzlyw.github.io/author/zhenliang-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhenliang-zhang/","section":"authors","summary":"I am a full-time research scientist at Beijing Institute for General Artificial Intelligence (BIGAI). I received my Ph.D. degree (‘19) from Beijing Institute of Technology advised by Prof. Yongtian Wang, and my research work was mainly about the information fusion of physical and virtual worlds, as well as the application of VR/AR/MR for robot learning.","tags":null,"title":"Zhenliang Zhang","type":"authors"},{"authors":null,"categories":null,"content":"coming soon \u0026hellip;\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"9f3733556ffbd03ac4e0242afbd1c949","permalink":"https://zzlyw.github.io/courses/srforai/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/srforai/","section":"courses","summary":"Learn how to use the symmetrical reality framework for developing general AI systems.","tags":null,"title":"Overview","type":"docs"},{"authors":["Zhenliang Zhang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"90b86ad14580830cd56e27c1504dcbd2","permalink":"https://zzlyw.github.io/publication/conf_vr21_symmetric_connition/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/conf_vr21_symmetric_connition/","section":"publication","summary":"In this paper, we discuss the cognition problem from some perspectives, i.e., attention, perception, pattern recognition, and communication.","tags":["Symmetrical Reality","Concept"],"title":"[VR 2021] Symmetrical cognition between physical humans and virtual agents","type":"publication"},{"authors":["Zhenliang Zhang","Yixin Zhu","Song-Chun Zhu"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1603584000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603584000,"objectID":"0a9f67a194f486a1c2104e6b47201461","permalink":"https://zzlyw.github.io/publication/conf_iros20_graph_based_robot_learning/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_iros20_graph_based_robot_learning/","section":"publication","summary":"We study the hierarchical knowledge transfer problem using a cloth-folding task, wherein the agent is first given a set of human demonstrations in the virtual world using an Oculus Headset, and later transferred and validated on a physical Baxter robot.","tags":["Symmetrical Reality","Machine Learning","Robot"],"title":"[IROS 2020] Graph-based hierarchical knowledge representation for robot task transfer from virtual to physical world","type":"publication"},{"authors":["Zhenliang Zhang","Xuejiao Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"3c194d703a12c28e58e3676f99167267","permalink":"https://zzlyw.github.io/publication/conf_ismar20_machine_intelligence/","publishdate":"2020-03-02T00:00:00Z","relpermalink":"/publication/conf_ismar20_machine_intelligence/","section":"publication","summary":"We introduce the contents of the symmetrical reality-based human-robot collaboration and interpret the humanrobot collaboration from the perspective of equivalent interaction.","tags":["Symmetrical Reality","Concept"],"title":"[ISMAR 2020] Machine intelligence matters: Rethink human-robot collaboration based on symmetrical reality","type":"publication"},{"authors":["Zhenliang Zhang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"f094772a851ed78552279ab9a9d93ddd","permalink":"https://zzlyw.github.io/publication/conf_ismar20_physical_commonsense/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/conf_ismar20_physical_commonsense/","section":"publication","summary":"We emphasize the bi-directional mechanical control within the symmetrical reality framework and why free wills of machines can break the common sense.","tags":["Symmetrical Reality","Concept"],"title":"[ISMAR 2020] Understanding physical common sense in symmetrical reality","type":"publication"},{"authors":["Jie Guo","Dongdong Weng","Hui Fang","Zhenliang Zhang","Jiaming Ping","Yue Liu","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"91a44e9a9d1b7d7a093bffdf6657c4ba","permalink":"https://zzlyw.github.io/publication/conf_vr20_long_term_immersion/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_vr20_long_term_immersion/","section":"publication","summary":"To investigate the effects of visual discomfort caused by long-term immersing in virtual environments (VEs), we conducted a comparative study to evaluate users’ visual discomfort in an eight-hour working rhythm and compared the differences between the VEs and the physical environments. Twenty-seven participants performed four different visual tasks with a head-mounted display (HMD) for the VE condition and with a monitor for the physical condition. Their subjective visual discomfort and objective oculomotor indicators were measured to evaluate their visual performances. The results show that the subjective visual fatigue symptoms, the objective pupil size, and the relative accommodation response vary across time for the two conditions, in which VEs affects visual fatigue the most compared to the physical environments. The results also show that pupil size is negatively related to subjective visual fatigue, and the long-term work based on displays only influences the maximum accommodation response of participants. This work is a supplement to the necessary but insufficient-researched field of visual fatigue in long-term immersing in VEs, which should be valuable to researchers involved in the evaluation of visual fatigue using HMDs.","tags":["Virtual Reality"],"title":"[VR 2020] Exploring the differences of visual discomfort caused by long-term immersion between virtual environments and physical environments","type":"publication"},{"authors":["Zhenliang Zhang","Jie Guo","Dongdong Weng","Yue Liu","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"ea70dce8ec41a6f0ac99d63c14a89788","permalink":"https://zzlyw.github.io/publication/conf_vr20_extract_knowledge_robot/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_vr20_extract_knowledge_robot/","section":"publication","summary":"We study the knowledge transfer problem by training the task of folding clothes in the virtual world using an Oculus Headset and validating with a physical Baxter robot. We argue such complex transfer is realizable if an abstract graph-based knowledge representation is adopted to facilitate the process. An And-Or-Graph (AOG) grammar model is introduced to represent the knowledge, which can be learned from the human demonstrations performed in the Virtual Reality (VR), followed by the case analysis of folding clothes represented and learned by the AOG grammar model.","tags":["Virtual Reality","Robot"],"title":"[VR 2020] Extracting and transferring hierarchical knowledge to robots using virtual reality","type":"publication"},{"authors":["Zhenliang Zhang","Dongdong Weng","Jie Guo","Yue Liu","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1573257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573257600,"objectID":"ccb14f8e8f3af79a46c995bd15fdbec3","permalink":"https://zzlyw.github.io/publication/conf_iros19_hybrid_hand_interaction/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_iros19_hybrid_hand_interaction/","section":"publication","summary":"Human-computer interaction (HCI) plays an important role in the near-field mixed reality, in which the hand-based interaction is one of the most widely-used interaction modes, especially in the applications based on optical see-through head-mounted displays (OST-HMDs). In this paper, such interaction modes as gesture-based interaction (GBI) and physics-based interaction (PBI) are developed to construct a mixed reality system to evaluate the advantages and disadvantages of different interaction modes. The ultimate goal is to find an efficient hybrid paradigm for mixed reality applications based on OST-HMDs to deal with the situations that a single interaction mode cannot handle. The results of the experiment, which compares GBI and PBI, show that PBI leads to a better performance of users regarding their work efficiency in the proposed two tasks. Some statistical tests, including T-test and one-way ANOVA, have also been adopted to prove that the difference regarding the efficiency between different interaction modes is significant. Experiments for combining both interaction modes are put forward in order to seek a good experience for manipulation, which proves that the partially-overlapping style would help to improve work efficiency for manipulation tasks. The experimental results of the proposed two hand-based interaction modes and their hybrid forms can provide some practical suggestions for the development of mixed reality systems based on OST-HMDs.","tags":["Virtual Reality","Human-Computer Interaction"],"title":"[IROS 2019] Toward an efficient hybrid interaction paradigm for object manipulation in optical see-through mixed reality","type":"publication"},{"authors":["Jie Guo","Dongdong Weng","Zhenliang Zhang","Haiyan Jiang","Yue Liu","Yongtian Wang","Henry Been-Lirn Duh"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1571529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571529600,"objectID":"72f2e8a81c8f019cf075438bb6b8a163","permalink":"https://zzlyw.github.io/publication/conf_ismar19_mixed_reality_office/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_ismar19_mixed_reality_office/","section":"publication","summary":"In a mixed reality (MR) environment that combines the physical objects with the virtual environments, users' feelings are immersed in the virtual world, while their bodies remain in the physical world. Compared to the purely physical environments, such characteristic has led to some special needs for users' long-term immersion. However, the deficiency needs that we have to face for long-term immersion still need further research. In this paper, we apply the theory of Maslow's Hierarchy of Needs (MHN) to guide the design of MR systems for long-term immersion. Taking the normal biological rhythm of human beings as the basic unit (24 hours), we propose the fundamental needs for long-term immersion in Ves through combining the theory of MHN with the special needs of virtual reality (VR). In order to verify whether those needs can satisfy users' long-term immersion, we design an MR office system for basic operations based on the theory of MHN. A long-term exposure experiment (duration of 8 hours) is designed to evaluate those needs by comparing the results with a physical work environment after a short-term preliminary study. The physiological and psychological effects are tested in both two environments and the deficiency needs for short-term immersion and long-term immersion are also compared. The results showed that the design based on the theory of MHN can support users' long-term immersion, which means that it can be a guideline for long-term use of MR systems.","tags":["Virtual Reality","Human-Computer Interaction"],"title":"[ISMAR 2019] Mixed reality office system based on maslow’s hierarchy of needs: Towards the long-term immersion in virtual environments","type":"publication"},{"authors":["Haiyan Jiang","Dongdong Weng","Zhenliang Zhang","Feng Chen"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1562803200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562803200,"objectID":"ee9d63d135999008ce91634c735b00f5","permalink":"https://zzlyw.github.io/publication/journal_sensors_hifinger/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/journal_sensors_hifinger/","section":"publication","summary":"We present a text entry technique called HiFinger, which is an eyes-free, one-handed wearable text entry technique for immersive virtual environments by thumb-to-fingers touch. This technique enables users to input text quickly, accurately, and comfortably with the sense of touch and a two-step input mode. It is especially suitable for mobile scenarios where users need to move (such as walking) in virtual environments. Various input signals can be triggered by moving the thumb towards ultra-thin pressure sensors placed on other fingers. After acquiring the comfort range of the touch between the thumb and other fingers, six placement modes for text entry are designed and tested, resulting in an optimal placement mode that leverages six pressure sensors for the text entry and two for the control function. A three-day study is conducted to evaluate the proposed technique, and experimental results show that novices can achieve an average text entry efficiency of 9.82 words per minute (WPM) in virtual environments based on head-mounted displays after a training period of 25 min.","tags":["Human-Computer Interaction"],"title":"[Sensors 2019] HiFinger: One-handed text entry technique for virtual environments based on touches between fingers","type":"publication"},{"authors":["Hangxin Liu","Zhenliang Zhang","Xu Xie","Yixin Zhu","Yue Liu","Yongtian Wang","Song-Chun Zhu"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1558310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558310400,"objectID":"41653ca13b3870752756881da67d2902","permalink":"https://zzlyw.github.io/publication/conf_icra19_vr_glove/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_icra19_vr_glove/","section":"publication","summary":"This paper presents a design that jointly provides hand pose sensing, hand localization, and haptic feedback to facilitate real-time stable grasps in Virtual Reality (VR). The design is based on an easy-to-replicate glove-based system that can reliably perform (i) a high-fidelity hand pose sensing in real time through a network of 15 IMUs, and (ii) the hand localization using a Vive Tracker. The supported physicsbased simulation in VR is capable of detecting collisions and contact points for virtual object manipulation, which drives the collision event to trigger the physical vibration motors on the glove to signal the user, providing a better realism inside virtual environments. A caging-based approach using collision geometry is integrated to determine whether a grasp is stable. In the experiment, we showcase successful grasps of virtual objects with large geometry variations. Comparing to the popular LeapMotion sensor, we demonstrate the proposed glove-based design yields a higher success rate in various tasks in VR. We hope such a glove-based system can simplify the data collection of human manipulations with VR.","tags":["Virtual Reality","Robot"],"title":"[ICRA 2019] High-fidelity grasping in virtual reality using a glove-based system","type":"publication"},{"authors":["Xu Xie","Hangxin Liu","Zhenliang Zhang","Yuxing Qiu","Feng Gao","Siyuan Qi","Yixin Zhu","Song-Chun Zhu"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1558310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558310400,"objectID":"9273464037e8c74495142d5bf3ec2887","permalink":"https://zzlyw.github.io/publication/conf_turc19_vrgym/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_turc19_vrgym/","section":"publication","summary":"We propose VRGym, a virtual reality testbed for realistic human-robot interaction. Different from existing toolkits and virtual reality environments, the VRGym emphasizes on building and training both physical and interactive agents for robotics, machine learning, and cognitive science.","tags":["Virtual Reality","Robot","Machine Learning"],"title":"[TURC 2019] VRGym: A virtual testbed for physical and interactive AI","type":"publication"},{"authors":null,"categories":null,"content":"Symmetrical reality is a new area for human-robot interaction across physical and virtual worlds.\n","date":1553644800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553644800,"objectID":"35e5b2642a06549052401f88f2553471","permalink":"https://zzlyw.github.io/project/sr-project/","publishdate":"2019-03-27T00:00:00Z","relpermalink":"/project/sr-project/","section":"project","summary":"A Project of Symmetrical Reality.","tags":["Symmetrical Reality","Concept"],"title":"Symmetrical Reality Project","type":"project"},{"authors":["Jie Guo","Dongdong Weng","Zhenliang Zhang","Yue Liu","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1553040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553040000,"objectID":"17f5380ff793d70587090e97da23cbc4","permalink":"https://zzlyw.github.io/publication/conf_vr19_evaluation_mhn/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_vr19_evaluation_mhn/","section":"publication","summary":"Long-term exposure to VR will become more and more important, but what we need for long term immersion to meet users fundamental needs is still under-researched. In this paper, we apply the theory of Maslows Hierarchy of Needs to guide the design of VR for longterm immersion based on the normal biological rhythm of human beings (24 hours). An office environment is designed to verify those needs. The efficiency, the physical and the psychological effects of this VR office system are tested. The results show that the VR office environment is as comfortable as the physical environment at short-term immersion and it can support users basic immersion. It means that the Maslows Hierarchy of Needs can be a guideline for long-term immersion.","tags":["Virtual Reality","Human-Computer Interaction"],"title":"[VR 2019] Evaluation of maslows hierarchy of needs on long-term use of HMDs - A case study of office environment","type":"publication"},{"authors":["Zhenliang Zhang","Cong Wang","Dongdong Weng","Yue Liu","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1553040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553040000,"objectID":"aaa5635a45220331ead8961205b5dc27","permalink":"https://zzlyw.github.io/publication/conf_vr19_symmetrical_reality/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_vr19_symmetrical_reality/","section":"publication","summary":"In this paper, we review the background of physical reality, virtual reality, and some traditional mixed forms of them. Based on the current knowledge, we propose a new unified concept called symmetrical reality to describe the physical and virtual world in a unified perspective. Under the framework of symmetrical reality, the traditional virtual reality, augmented reality, inverse virtual reality, and inverse augmented reality can be interpreted using a unified presentation. We analyze the characteristics of symmetrical reality from two different observation locations (i.e., from the physical world and from the virtual world), where all other forms of physical and virtual reality can be treated as special cases of symmetrical reality.","tags":["Symmetrical Reality","Concept"],"title":"[VR 2019] Symmetrical reality: Toward a unified framework for physical and virtual reality","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://zzlyw.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Jie Guo","Dongdong Weng","Zhenliang Zhang","Yue Liu","Henry B.L. Duh","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"8f19f0d662a9c0283d629aed50414707","permalink":"https://zzlyw.github.io/publication/journal_sid_visual_fatigue/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/journal_sid_visual_fatigue/","section":"publication","summary":"During continuous use of displays, a short rest can relax users' eyes and relieve visual fatigue. As one of the most important devices of virtual reality, head‐mounted displays (HMDs) can create an immersive 3D virtual world. When users have a short rest during the using of HMDs, they will experience a transition from virtual world to real world. In order to investigate how this change affects users' eye condition, we designed a 2 × 2 experiment to explore the effects of short rest during continuous using of HMDs and compared the results with those of 2D displays. The Visual Fatigue Scale, critical flicker frequency, visual acuity, pupillary diameter, and accommodation response of 80 participants were measured to assess the subject's performance. The experimental results indicated that a short rest during the using of 2D displays could significantly reduce users' visual fatigue. However, the experimental results of using HMDs showed that short rest during continuous using of HMD induced more severe symptoms of subjectively visual discomfort, but reduced the objectively visual fatigue.","tags":["Human-Computer Interaction","Virtual Reality"],"title":"[SID 2019] Subjective and objective evaluation of visual fatigue caused by continuous and discontinuous use of HMDs","type":"publication"},{"authors":["Zhenliang Zhang","Yue Li","Jie Guo","Dongdong Weng","Yue Liu","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"71f6b9a40ba41ee4b41eed16c8fecfbe","permalink":"https://zzlyw.github.io/publication/journal_sid_editable_reality/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/journal_sid_editable_reality/","section":"publication","summary":"Building a human-centered editable world can be fully realized in a virtual environment. Both mixed reality (MR) and virtual reality (VR) are feasible solutions to support the attribute of edition. Based on the current development of MR and VR, we present the vision-tangible interactive display method and its implementation in both MR and VR. We address the issue of MR and VR together because they are similar regarding the proposed method. The editable mixed and virtual reality system is useful for studies, which exploit it as a platform. In this paper, we construct a virtual reality environment based on the Oculus Rift, and an MR system based on a binocular optical see-through head-mounted display. In the MR system about manipulating the Rubik's cube, and the VR system about deforming the virtual objects, the proposed vision-tangible interactive display method is utilized to provide users with a more immersive environment. Experimental results indicate that the vision-tangible interactive display method can improve the user experience and can be a promising way to make the virtual environment better.","tags":["Human-Computer Interaction","VIrtual Reality","Mixed Reality"],"title":"[SID 2019] Vision-tangible interactive display method for mixed and virtual reality: Toward the human-centered editable reality","type":"publication"},{"authors":["Haiyan Jiang","Dongdong Weng","Zhenliang Zhang","Yihua Bao","Yufei Jia","Mengman Nie"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"bd77edd108ec3c8a51ac991a4d8bb1d4","permalink":"https://zzlyw.github.io/publication/conf_ismar18_hikeyb/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_ismar18_hikeyb/","section":"publication","summary":"Text entry is an imperative issue to be addressed in current entry systems for virtual environments (VEs). The entry method using a physical keyboard is still the most dominant choice for an efficient interaction regarding text entry. In this paper, we propose a typing system with a style of mixed reality, which is called HiKeyb, and it possesses a similar high-efficiency with the single physical keyboard in the real environment. The HiKeyb system consists of a depth camera, a pose tracking module, a head-mounted display (HMD), a QWERTY keyboard and a black table mat. This system can guarantee the entry efficiency and the amenity by not only introducing the force feedback from a movable physical keyboard, but also improving the immersion with the real hand image. In addition, the infrared absorption material helps improve the robustness of the system against different lighting environments. Experiments have proved that users wearing HMDs in Virtual Phrases session can achieve an entry rate of 23.1 words per minute and an error rate of 2.76%, and the rate ratio of virtual reality to real world is 78% when typing phrases. Besides, we find that the proposed system can provide a relatively close entry efficiency to that using a pure physical keyboard in the real environment.","tags":["Virtual Reality","Human-Computer Interaction"],"title":"[ISMAR 2018] HiKeyb: High-efficiency mixed reality system for text entry","type":"publication"},{"authors":["Zhenliang Zhang","Dongdong Weng","Haiyan Jiang","Yue Liu","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"6208c4f89a3605f2e19fbb8064bfb49f","permalink":"https://zzlyw.github.io/publication/conf_ismar18_iar/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_ismar18_iar/","section":"publication","summary":"We propose a framework called inverse augmented reality (IAR) which describes the scenario that a virtual agent living in the virtual world can observe both virtual objects and real objects. This is different from the traditional augmented reality. The traditional virtual reality, mixed reality and augmented reality are all generated for humans, i.e., they are human-centered frameworks. On the contrary, the proposed inverse augmented reality is a virtual agent-centered framework, which represents and analyzes the reality from a virtual agent's perspective. In this paper, we elaborate the framework of inverse augmented reality to argue the equivalence of the virtual world and the physical world regarding the whole physical structure.","tags":["Mixed Reality","Concept","Symmetrical Reality"],"title":"[ISMAR 2018] Inverse augmented reality: A virtual agent's perspective","type":"publication"},{"authors":["Zhenliang Zhang","Yue Li","Jie Guo","Dongdong Weng","Yue Liu","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"4330feb236b92c0514137fec2c3cf758","permalink":"https://zzlyw.github.io/publication/conf_photonicsasia19_depth_aware/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_photonicsasia19_depth_aware/","section":"publication","summary":"Vision-tangible mixed reality (VTMR) is a further development of the traditional mixed reality. It provides an experience of directly manipulating virtual objects at the perceptual level of vision. In this paper, we propose a mixed reality system called “VTouch”. VTouch is composed of an optical see-through head-mounted display (OST-HMD) and a depth camera, supporting a direct 6 degree-of-freedom transformation and a detailed manipulation of 6 sides of the Rubik’s cube. All operations can be performed based on the spatial physical detection between virtual and real objects. We have not only implemented a qualitative analysis of the effectiveness of the system by a functional test, but also performed quantitative experiments to test the effects of depth occlusion. In this way, we put forward basic design principles and give suggestions for future development of similar systems. This kind of mixed reality system is significant for promoting the development of the intelligent environment with state-of-the-art interaction techniques.","tags":["Mixed Reality","Human-Computer Interaction"],"title":"[PhotonicsAsia 2018] Depth-aware interactive display method for vision-tangible mixed reality","type":"publication"},{"authors":["Zhenliang Zhang","Yue Li","Jie Guo","Dongdong Weng","Yue Liu","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"62c1af47e5bb1d5810762c431f2b6c79","permalink":"https://zzlyw.github.io/publication/journal_sid_lac/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/journal_sid_lac/","section":"publication","summary":"Calibration accuracy is one of the most important factors to affect the user experience in mixed reality applications. For a typical mixed reality system built with the optical see‐through head‐mounted display, a key problem is how to guarantee the accuracy of hand–eye coordination by decreasing the instability of the eye and the head‐mounted display in long‐term use. In this paper, we propose a real‐time latent active correction algorithm to decrease hand–eye calibration errors accumulated over time. Experimental results show that we can guarantee an effective calibration result and improve the user experience with the proposed latent active correction algorithm. Based on the proposed system, experiments about virtual buttons are also designed, and the interactive performance regarding different scales of virtual buttons is presented. Finally, a direct physics‐inspired input method is constructed, which shares a similar performance with the gesture‐based input method but provides a lower learning cost due to its naturalness.","tags":["Human-Computer Interaction","Mixed Reality"],"title":"[SID 2018] Task-driven latent active correction for physics-inspired input method in near-field mixed reality applications","type":"publication"},{"authors":["Zhenliang Zhang","Dongdong Weng","Jie Guo","Yue Liu","Yongtian Wang","Hua Huang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"a5e31f885b69d8e9a1ee45c7677c506a","permalink":"https://zzlyw.github.io/publication/journal_oe_osthmd/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/journal_oe_osthmd/","section":"publication","summary":"Single point active alignment method is a widely used calibration method for optical-see-through head-mounted displays (OST-HMDs) since its appearance. It always requires high-accuracy alignment for data acquisition, and the collected data affect the calibration accuracy to a large extent. However, there are often many kinds of alignment errors occurring in the calibration process. These errors may contain random errors of manual alignment and system errors of the fixed eye-HMD model. To tackle these problems, we first leverage a random sample consensus approach to recurrently decrease the random error of the collected data sequence and use a region-induced data enhancement method to reduce the system error. We design a typical framework to enhance the data acquisition for calibration, sequentially reducing the random error and the system error. Experimental results show that the proposed method can significantly make the calibration more robust due to the elimination of sampling points with large errors. At the same time, the calibration accuracy can be increased by the proposed dynamic eye-HMD model that takes the eye movement into consideration. The improvement about calibration should be significant to promote the applications based on OST-HMDs.","tags":["Human-Computer Interaction","Mixed Reality"],"title":"[Optical Engineering 2018] Enhancing data acquisition for calibration of optical see-through head-mounted displays","type":"publication"},{"authors":["Hongling Sun","Yue Liu","Zhenliang Zhang","Xiaoxu Liu","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"3a898a29464e241ca22ff5e79e6abfe7","permalink":"https://zzlyw.github.io/publication/conf_chinesechi18_remote_collaboration/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_chinesechi18_remote_collaboration/","section":"publication","summary":"This paper details the design, implementation and an initial evaluation of a collaborative platform named OptoBridge, which is aimed at enhancing remote guidance and skill acquisition for spatially distributed users. OptoBridge integrates augmented reality (AR), gesture interaction with video mediated communication and is preliminarily applied to the experimental teaching of the adjustment task with Michelson interferometer. An exploratory study has been conducted to qualitatively and quantitatively evaluate the extent to which different viewpoints affect the student's sense of presence, task performance, learning outcomes and subjective feelings in the remote collaborative augmented environment. 16 students from local universities have participated in the evaluation. The result shows the influence of two different viewpoints and indicates that OptoBridge can effectively support remote guidance and enhance the collaborators' experience.","tags":["Mixed Reality","Remote Collaboration"],"title":"[ChineseCHI 2018] Employing different viewpoints for remote guidance in a collaborative augmented environment","type":"publication"},{"authors":["Zhenliang Zhang","Benyang Cao","Dongdong Weng","Yue Liu","Yongtian Wang","Hua Huang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1521504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521504000,"objectID":"036872e07d5e7d78bf5c6512b20d34fd","permalink":"https://zzlyw.github.io/publication/conf_vr18_hand_interaction/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_vr18_hand_interaction/","section":"publication","summary":"Hand-based interaction is one of the most widely-used interaction modes in the applications based on optical see-through head-mounted displays (OST-HMDs). In this paper, such interaction modes as gesture-based interaction (GBI) and physics-based interaction (PBI) are developed to construct a mixed reality system to evaluate the advantages and disadvantages of different interaction modes for near-field mixed reality. The experimental results show that PBI leads to a better performance of users regarding their work efficiency in the proposed tasks. The statistical analysis of T-test has been adopted to prove that the difference of efficiency between different interaction modes is significant.","tags":["Mixed Reality","Human-Computer Interaction","Head-Mounted Display"],"title":"[VR 2018] Evaluation of hand-based interaction for near-field mixed reality with optical see-through head-mounted displays","type":"publication"},{"authors":["Zhenliang Zhang","Benyang Cao","Jie Guo","Dongdong Weng","Yue Liu","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1521504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521504000,"objectID":"64f479fc38ee3aa0a0ec46085301248d","permalink":"https://zzlyw.github.io/publication/conf_vr18_ivr/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_vr18_ivr/","section":"publication","summary":"Since artificial intelligence has been integrated into virtual reality, a new branch of virtual reality, which is called inverse virtual reality (IVR), is created. A typical IVR system contains both the intelligence-driven virtual reality and the physical reality, thus constructing an intelligence-driven mutually mirrored world. We propose the concept of IVR, and describe the details about the definition, structure and implementation of a typical IVR system. The paral-lei living environment is proposed as a typical application of IVR, which reveals that IVR has a significant potential to extend the human living environment.","tags":["Virtual Reality","Concept","Symmetrical Reality"],"title":"[VR 2018] Inverse virtual reality: Intelligence-driven mutually mirrored world","type":"publication"},{"authors":["Zhenliang Zhang","Yue Li","Dongdong Weng","Yue Liu","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1521504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521504000,"objectID":"380730b2b22dc3d435d04f237b261371","permalink":"https://zzlyw.github.io/publication/conf_vr18_physics_input_method/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_vr18_physics_input_method/","section":"publication","summary":"Calibration accuracy is one of the most important factors to affect the user experience in mixed reality applications. For a typical mixed reality system built with the optical see-through head-mounted display (OST-HMD), a key problem is how to guarantee the accuracy of hand-eye coordination by decreasing the instability of the eye and the HMD in long-term use. In this paper, we propose a real-time latent active correction (LAC) algorithm to decrease hand-eye calibration errors accumulated over time. Experimental results show that we can successfully use the LAC algorithm to physics-inspired virtual input methods.","tags":["Mixed Reality","Human-Computer Interaction","Head-Mounted Display"],"title":"[VR 2018] Physics-inspired input method for near-field mixed reality applications using latent active correction","type":"publication"},{"authors":null,"categories":null,"content":"Robot Learning project is focused on training a robot with VR or MR approaches.\n","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"65b03173abd8586a5c02e82ae5c8f82d","permalink":"https://zzlyw.github.io/project/robot-learning-project/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/project/robot-learning-project/","section":"project","summary":"A Project of Robot Leraning.","tags":["Robot","Machine Learning"],"title":"Robot Learning Project","type":"project"},{"authors":["Zhenliang Zhang","Dongdong Weng","Jie Guo","Yue Liu","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1508457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508457600,"objectID":"217ea4247f18231a4eece3fba6d23d63","permalink":"https://zzlyw.github.io/publication/conf_ismar17_eye_observation_model/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_ismar17_eye_observation_model/","section":"publication","summary":"Single point active alignment method (SPAAM) has become the basic calibration method for optical-see-through head-mounted displays since its appearance. However, SPAAM is based on a simple static pinhole camera model that assumes a static relationship between the user's eye and the HMD. Such theoretic defects lead to a limitation in calibration accuracy. We model the eye as a dynamic pinhole camera to account for the displacement of the eye during the calibration process. We use region-induced data enhancement (RIDE) to reduce the system error in the acquisition process. The experimental results prove that the proposed dynamic model performs better than the traditional static model, and the RIDE method can help users obtain a more accurate calibration result based on the dynamic model, which improves the accuracy significantly compared to the standard SPAAM.","tags":["Mixed Reality","Head-Mounted Display"],"title":"[ISMAR 2017] An accurate calibration method for optical see-through head-mounted displays based on actual eye-observation model","type":"publication"},{"authors":["Zhenliang Zhang","Dongdong Weng","Yue Liu","Yongtian Wang","Xinjun Zhao"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1489968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489968000,"objectID":"8ec646efa6ee2491171e6858e7fd0744","permalink":"https://zzlyw.github.io/publication/conf_vr17_ride/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_vr17_ride/","section":"publication","summary":"The most commonly used single point active alignment method (SPAAM) is based on a static pinhole camera model, in which it is assumed that both the eye and the HMD are fixed. This leads to a limitation for calibration precision. In this work, we propose a dynamic pinhole camera model according to the fact that the human eye would experience an obvious displacement over the whole calibration process. Based on such a camera model, we propose a new calibration data acquisition method called the region-induced data enhancement (RIDE) to revise the calibration data. The experimental results prove that the proposed dynamic model performs better than the traditional static model in actual calibration.","tags":["Virtual Reality","Head-Mounted Display"],"title":"[VR 2017] RIDE: Region-induced data enhancement method for dynamic calibration of optical see-through head-mounted displays","type":"publication"},{"authors":["Hongling Sun","Zhenliang Zhang","Yue Liu","Henry BL Duh"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"7c19565509267d080d11c97469e96d76","permalink":"https://zzlyw.github.io/publication/conf_ozchi16_optobridge/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_ozchi16_optobridge/","section":"publication","summary":"In this paper an experimental teaching platform named OptoBridge is presented which supports the sharing of the collaborative space for spatially distributed users to assist skill acquisition. The development of OptoBridge is based on augmented reality (AR) and integrates free-hand gesture interactions with the video mediated communication. The prototype is preliminarily applied in the optics field to promote skill execution in the case of the Michelson interferometer. OptoBridge enables the remote teacher to monitor the experimental scenario as well as the detailed optical phenomena through the transmitted video captured on the local side. Meanwhile the local learner equipped with the optical see-through head-mounted display (OSTHMD) can be indicated by virtual hands and augmented annotations controlled by the teacher's gestures and follow the guidance to get their skills practiced. The implementation of OptoBridge is also presented, aimed at providing a more engaging and efficient approach for remote skill teaching.","tags":["Mixed Reality","Remote Collaboration"],"title":"[OzCHI 2016] OptoBridge: Assisting skill acquisition in the remote experimental collaboration","type":"publication"},{"authors":["Zhenliang Zhang","Dongdong Weng","Yue Liu","Yongtian Wang"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1472688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472688000,"objectID":"bcbf1f02b2ca53177eabc6182088603d","permalink":"https://zzlyw.github.io/publication/conf_icvrv16_calibration/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_icvrv16_calibration/","section":"publication","summary":"With the rapid development of Virtual and Augmented Reality systems, it becomes more and more important to develop an efficient calibration method for optical see-through head-mounted displays (OST-HMDs). In this paper, a modular calibration framework with two calibration phases is proposed. In the first phase, an eye-involved equivalent camera model is proposed in order to compute the spatial position of the human eye directly, in the second phase, the gesture information is integrated to the system with a depth camera. In addition, a fast correction algorithm is introduced to ensure that the calibration result work for new users without additional complex recalibration procedures. The precision of the proposed modular calibration and optimization method is evaluated, and the result shows that the proposed method can simplify the recalibration procedures for OST-HMDs.","tags":["Mixed Reality","Head-Mounted Display"],"title":"[ICVRV 2016] A modular calibration framework for 3D interaction system based on optical see-through head-mounted displays in augmented reality","type":"publication"},{"authors":["Zhenliang Zhang","Dongdong Weng","Yue Liu","Xiang Li"],"categories":null,"content":" Click the Cite button above to import publication metadata into your reference management software. ","date":1429488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1429488000,"objectID":"401b073619cf1979d52f5d61a12822e9","permalink":"https://zzlyw.github.io/publication/conf_icopen15_3dar/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/conf_icopen15_3dar/","section":"publication","summary":"The combination of health and entertainment becomes possible due to the development of wearable augmented reality equipment and corresponding application software. In this paper, we implemented a fast calibration extended from SPAAM for an optical see-through head-mounted display (OSTHMD) which was made in our lab. During the calibration, the tracking and recognition techniques upon natural targets were used, and the spatial corresponding points had been set in dispersed and well-distributed positions. We evaluated the precision of this calibration, in which the view angle ranged from 0 degree to 70 degrees. Relying on the results above, we calculated the position of human eyes relative to the world coordinate system and rendered 3D objects in real time with arbitrary complexity on OSTHMD, which accurately matched the real world. Finally, we gave the degree of satisfaction about our device in the combination of entertainment and prevention of cervical vertebra diseases through user feedbacks.","tags":["Mixed Reality","Head-Mounted Display"],"title":"[ICOPEN 2015] 3D optical see-through head-mounted display based augmented reality system and its application","type":"publication"},{"authors":null,"categories":null,"content":"Mixed Reality project is focused on the wearable MR devices, including algorithms, systems, and applications.\n","date":1362096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1362096000,"objectID":"f18fcafe4f035149c0d028aec3935527","permalink":"https://zzlyw.github.io/project/mr-project/","publishdate":"2013-03-01T00:00:00Z","relpermalink":"/project/mr-project/","section":"project","summary":"A Project of Mixed Reality.","tags":["Mixed Reality"],"title":"Mixed Reality Project","type":"project"}]